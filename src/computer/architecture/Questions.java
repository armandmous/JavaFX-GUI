/**
 * @author: Armand Moussaouyi
 * @date: January 03, 2018
 * 
 * @description: This object is being used to store our data
 */
package computer.architecture;

public class Questions {
    private final String[] questions = {
        "1.\tThe Architecture course is not about (2)", 
        "2.\tDeveloping computer technology has resulted in important improvements\n\thave been made in (2)",
        "3.\tIn the past 10 years, single processor performance has (3)",
        "4.\tWhat sort of parallelism isn't relevant to processor performance (5)",
        "5.\tWhich is not a class of computer? (6)",
        "6.\tBy far, the costliest down time is in the domain of (9)",
        "7.\tData-level parallelism is a feature found in (10)",
        "8.\tBefore multiple cores came along, which type of processing was\n\tcommon (11)",
        "9.\tToday, computer architecture targets (11)",
        "10.\tWhat is not a feature of Instruction Set Architecture (13)",
        "11.\tInstruction sets which are limited in size are called (14)",
        "12.\tOne feature of the RISC-V instruction set is that (16)",
        "13.\tWhich is not a significant market need for computers? (17)",
        "14.\tThe \"law\" that describes the periodic reduction in feature size\n\ton chips is (18)",
        "15.\tRegarding computer chip capability, which has improved the most in\n\tthe last 20 years? (20)",
        "16.\tDennard scaling is (21)",
        "17.\tMany processors limit power consumption by (23)",
        "18.\tHow can a CPU be \"cooled off\" while still operating? (23)",
        "19.\tIf you lower the processor speed (25)",
        "20.\tWhat happened to CPUs produced after the early (year) 2000's? (26)",
        "21.\tWhat is static power? (29)",
        "22.\tStatic power can be changed while the CPU is operating (29)",
        "23.\tWhat will drive the cost of a processor chip down? (29)",
        "24.\tBecause of their unique designs, processors follow a different\n\tprice profile over time compared with DRAM chips (29)",
        "25.\tThe die yield affects (31)",
        "26.\tDependability is composed of two things (36)",
        "27.\tMeasuring reliability depends on the definitions of (36)",
        "28.\tWhich is a typical performance metric? (39)",
        "29.\tWhich timing statistic includes Operating system and I/O calls? (39)",
        "30.\tThe speedup of X relative to Y is (39)",
        "31.\tWhat is not a difficulty with benchmarks? (40)",
        "32.\tWhat are benchmark suites? (41)",
        "33.\tAggregation of performance results (speedups) is done using (46)",
        "34.\tWhich is not a principle of computer design? (48)",
        "35.\tAn average CPI of groups of instructions with different CPIs can be\n\tcalculated by (53)",
        "36.\tMultiplying the number of instructions times CPI yields (53)",
        "37.\tWhich of the following yields CPU time? (53)",
        "38.\tWhich is not an example of an exponential rule which is no longer\n\tthe case? (58)",
        "39.\tConstant power dissipation for constant chip area was (58)",
        "40.\tCurrent thoughts on the limit of feature size puts the limit at (58)",
        "41.\tProgrammers now represent the chief limitation in computer performance (60)",
        "42.\tAn application cannot effectively use an improvement provided by a\n\tprocessor.  This is an example of (60)",
        "43.\tPerformance increases lead to energy efficiency (60)",
        "44.\tWhy do benchmark suites not maintain their validity over time? (60)",
        "45.\tThe top of the memory hierarchy is composed of (78)",
        "46.\tIn the memory hierarchy, main memory is   (78)",
        "47.\tA proper memory hierarchy which leverages spatial and temporal locality\n\tensures (78)",
        "48.\tLike the desktop market, personal mobile device processors have large L3\n\tcaches (79)",
        "49.\tOver the last 35 years   (80)",
        "50.\tMulticore processors (as opposed to unicore processors) (80)",
        "51.\tMulticore caches have all these characteristics except (80)",
        "52.\tCompared to desktop processors, server processors (80)",
        "53.\tWith n-way associative cache (81)",
        "54.\tThe set into which a cache block is placed depends on (81)",
        "55.\tA direct-mapped cache has   (81)",
        "56.\tTHe cache write strategy which updates lower levels simultaneously is (81)",
        "57.\tWhich is not a type of cache miss? (81)",
        "58.\tHiding miss latency can be achieved by (82)",
        "59.\tHigher cache associativity does not mean (83)",
        "60.\tThe reason static random access memory is used for cache is (84)",
        "61.\tMain memory is organized to (84)",
        "62.\tDRAM is slow because of (85)",
        "63.\tWhich is not a DRAM optimization designed for better performance? (88)",
        "64.\tSuccessive DDR levels are distinguished by all except (89)",
        "65.\tGraphics DDR can use a higher clock rate because (90)",
        "66.\tA new way of speeding DRAM access time is   (91)",
        "67.\tFlash memory is   (92)",
        "68.\tA significant flash memory limitation is   (92)",
        "69.\tDRAM dependability can be had by   (93)",
        "70.\tWhich is not a cache optimization?   (94)",
        "71.\tThe fastest access time is with   (96)",
        "72.\tAssuming reads are equally fast, which consumes the most energy  (97)",
        "73.\tWhat is the effect of way prediction?   (98)",
        "74.\tA downside of pipelined cache is that it   (99)",
        "75.\tMulti-banked cache is characterized by   (100)",
        "76.\tA non-blocking cache   (102)",
        "77.\tIf, on an L1 miss, the data from L2 is sent first before the rest of the\n\tblock, we are using   (104)",
        "78.\tA dirty block has been evicted from L1 and is in the write buffer going\n\tto L2.  The processor issues a write to the evicted block.  What happens with\n\ta merged write buffer?   (105)",
        "79.\tProcessing a matrix operation in chunks that fit into cache is an\n\toptimization called   (108)",
        "80.\tHow can prefetching be done?   (110)",
        "81.\tVirtual memory schemes provide process protection   (119)",
        "82.\tFor virtual memory to be effective, there must be hardware support in\n\tthe processor   (119)",
        "83.\tThe job of the Translation Lookaside Buffer (TLB) is to   (119)",
        "84.\tVirtual machines and virtual memory are the same thing   (121)",
        "85.\tWhat is the main purpose of a virtual machine?   (121)",
        "86.\tThe virtual machine monitor   (122)",
        "87.\tWhen a virtual Machine Monitor handles Virtual Memory   (123)",
        "88.\tIf a processor hardware supplies virtualization support   (126)",
        "89.\tThe purpose of an instruction fetch unit is to   (127)",
        "90.\tIf a speculated instruction throws an exception   (128)",
        "91.\tSpecial instruction caches   (128)",
        "92.\tWhat is dynamic scheduling?   (191)",
        "93.\tWhat is a disadvantage of dynamic scheduling   (191)",
        "94.\tDynamic Scheduling causes   (193)",
        "95.\tAn antidependence is remedied with   (193)",
        "93.\tWhat is Tomasulo's approach?   (195)",
        "94.\tTomasulo's approach makes use of   (195)",
        "95.\tTomasulo scheduling is capable of   (195)",
        "96.\tAn antidependence is   (196)",
        "97.\tTomasulo's approach automatically does   (197)",
        "98.\tWhen using Tomasulo scheduling,   (197)",
        "99.\tWhich is not a step in Tomasulo's algorithm   (198)",
        "100.\tWhen an instruction is committed   (208)",
        "101.\tWhat does a reorder buffer do?   (209)",
        "102.\tWhat happens to results of mispredicted instructions   (209)",
        "103.\tWhen are exceptions on a speculative instruction handled?   (209)",
        "104.\tBecause load/store operations use different hardware than arithmetic\n\toperations, load/store instructions are handled in separate instruction queues   (210)",
        "105.\tWe achieve a CPI of less than 1 in all these ways except   (218)",
        "106.\tA Very Long Instruction Word   (220)",
        "107.\tWhich method is not used in modern microarchitectures to achieve\n\tperformance?   (222)",
        "108.\tIt can take longer to issue an instruction than to execute it   (222)",
        "109.\tIn bundles of instructions issued in multiple issue processors (224)",
        "110.\tWhat does the Branch-Target Buffer add to branch prediction?  (229)",
        "111.\tWhat does Branch-Folding add to branch prediction   (229)",
        "112.\tif a program executes a branch, what benefit is there to having\n\tthe branch target instruction available without having to fetch it?   (229)",
        "113.\tA small stack which contains the address of the instruction after\n\ta procedure call (such as the MIPS jal or x-86 call instruction) is the   (230)",
        "114.\tIf a return address predictor has more entries than the maximum\n\tcall depth in a program   (233)",
        "115.\tWhich capability is not included in an integrated instruction fetch\n\tunit?   (233)",
        "116.\tReorder buffers can accomplish which other task?   (234)",
        "117.\tWhat's the problem with speculating through multiple branches?  (237)",
        "118.\tAggressive speculation   (237)",
        "119.\tIn which type of application are branch misprediction rates high? (239)",
        "120.\tHardware speculation is better when   (240)",
        "121.\twhen executing speculative instructions   (241)",
        "122.\tA uniprocessor (or a core) can usually support ____ thread(s)\n\tfrom the same process   (242)",
        "123.\tThere is usually enough parallelism in the threads running on one\n\tcore to hide off-chip latency   (242)",
        "124.\tEach thread running on a core requires its own cache   (242)",
        "125.\tThe _least_ energy efficient multithreading model is   (244)",
        "126.\tThe _most_ energy efficient multithreading model is   (244)",
        "127.\tThe simultaneous multithreading model can execute instructions\n\tfrom different threads in a single cycle, but it can only issue\n\tinstructions from the same thread in a cycle.   (245)",
        "128.\tEnergy efficiency gains with multithreaded Java benchmark programs\n\ttend to be low because   (246)",
        "129.\tNon-branching, integer instructions on the ARM\n\tCortex-A53 require ____ pipeline stages   (248)",
        "130.\tThe Cortex-A53 floating point unit has _____ stages   (249)",
    };
    
    private final String[] answers = {
        "A. Brands\nB. Performance\nC. Market\nD. Economics",
        "A. Durability (E.g., damage in dropping)\nB. Amount of parallelism in programs\nC. Feature Size\nD. Increasing number of instructions\nE. Clock Speed\nF. Chip weight\nG. Security",
        "A. Become unmeasurable\nB. Skyrocketed\nC. Dropped\nD. Leveled off",
        "A. Request-level parallelism\nB. Thread-level parallelism\nC. Circuit-level parallelism\nD. Data-level parallelism",
        "A. Personal Mobile Device\nB. Server\nC. Laptop\nD. Desktop",
        "A. Real estate\nB. Media outlets\nC. Brokerage services\nD. Search engines",
        "A. I/O devices\nB. Processors or memory\nC. Programs or applications\nD. Instructions",
        "A. Single Instruction, single data\nB. Multiple Instruction, multiple data\nC. Multiple Instruction, single data\nD. Single Instruction, multiple data",
        "A. Specific users\nB. Specific applications\nC. Specific markets\nD. Specific chip sets",
        "A. Addressing modes\nB. Types and size of operands\nC. Supply voltage\nD. Memory addressing",
        "A. CISC\nB. NISC\nC. ARM\nD. RISC",
        "A. All instructions are the same length\nB. Instructions can vary in length\nC. There are at least a dozen types of instructions\nD. The operation code appears in different positions in different instructions",
        "A. Case color\nB. Power\nC. Speed\nD. Number of users",
        "A. Amdahl's Law\nB. Boyle's Law\nC. Ohm's Law\nD. Moore's Law",
        "A. Power Consumption\nB. Bandwidth\nC. I/O delays\nD. Latency",
        "A. Regardless of feature size, an area on a chip dissipates the same amount of power\nB. Computers will be replaced every two years\nC. The amount of improvement due to a new feature is application dependent\nD. The feature size in chip lithography reduces by two every two years",
        "A. Shutting down periodically\nB. Changing clock speed\nC. Changing supply voltage\nD. Changing instruction length",
        "A. Lower the clock frequency\nB. Move the process to another core\nC. Decrease the capacitive load\nD. Lower the CPU voltage",
        "A. You reduce both power and energy\nB. You reduce energy, but not power\nC. Power and energy consumption are unaffected, the processor just runs slower\nD. You reduce power, but not energy",
        "A. The chips became so fast that more than one program could be run at once\nB. An increasing number of manufacturers drove down the prices of new chips\nC. Silicon chips hit the limit of heat that could be dissipated by air\nD. The photolithography process produced more transistors than the processor needed",
        "A. Volts / Amperes\nB. Volts x Amperes\nC. Volts + Amperes\nD. Volts - Amperes",
        "A. T\nB. F",
        "A. Increasing performance\nB. Increasing demand\nC. Increasing engineering cost\nD. Increasing yield",
        "A. F\nB. T",
        "A. The number of defects in a die\nB. The performance of the processor\nC. The cost of the die\nD. The size of the chip",
        "A. Availability and outage\nB. Reliability and integrity\nC. Reliability and vulnerability\nD. Reliability and availability",
        "A. Availability and outage\nB. Reliability and integrity\nC. Reliability and vulnerability\nD. Reliability and availability",
        "A. Cycles per instruction\nB. Number of instructions\nC. Clock rate\nD. Throughput",
        "A. System time\nB. Operating time\nC. CPU time\nD. Wall clock time",
        "A. Execution time(X) - Execution time (Y)\nB. Execution time(Y) / Execution time (X)\nC. Execution time(Y) - Execution time (X)\nD. Execution time(X) / Execution time (Y)",
        "A. Can compile them with optimization settings not normally allowed\nB. Many benchmark programs are unrealistic\nC. Can rewrite code to take advantages of processor-unique characteristics\nD. Some benchmark program require extensive data sets",
        "A. A standard series of machines used as performance references\nB. A set of hardware which measures system performance\nC. A hotel chain based in Mountain View, CA\nD. Sets of programs which cover a particular application space",
        "A. The mode of the distribution\nB. The arithmetic mean\nC. The median of the distribution\nD. The geometric mean",
        "A. Fail early\nB. Focus on the Common Case\nC. Take advantage of parallelism\nD. Use the principle of locality",
        "A. Dividing the total number of instructions by the total number of cycles to execute them\nB. Taking the average of the CPIs\nC. Finding the geometric mean of the CPIs\nD. Summing the products of the CPI of a class of instructions with the proportion of instructions of that class",
        "A. Clock cycles\nB. Average CPI\nC. Program length in instructions\nD. CPU time",
        "A. Number of cycles times clock frequency\nB. Clock cycle time divided by number of cycles\nC. Number of cycles divided by clock frequency\nD. Number of cycles divided by clock cycle time",
        "A. Dennard Scaling\nB. Amdahl's Law\nC. Disk Capacity growth\nD. Moore's Law",
        "A. Moore's Law\nB. Dennard Scaling\nC. Amdahl's Law\nD. Disk Capacity growth",
        "A. 3 nm\nB. 11 nm\nC. 7 nm\nD. 1 nm",
        "A. T\nB. F",
        "A. Amdahl's Law\nB. Belady's anomaly\nC. Moore's Law\nD. Dennard's Theorem",
        "A. F\nB. T",
        "A. Modern processors can no longer execute older benchmark programs\nB. Compiler optimizations, which aren't allowed to be used on benchmark programs,\n   improve performance without improving the benchmark\nC. Programmers are always writing new benchmark software\nD. Older benchmark programs often contain code which is rendered less efficient by\n   modern compiler optimizations",
        "A. Slow, inexpensive memory\nB. Fast, expensive memory\nC. Slow, expensive memory\nD. Fast, inexpensive memory",
        "A. Slow, inexpensive memory\nB. Slow, expensive memory\nC. Fast, inexpensive memory\nD. Fast, expensive memory",
        "A. The most efficient use of disk storage\nB. That all data can be held in main memory\nC. The least amount of energy will be consumed in running a program\nD. That almost all memory references are found in upper levels of the hierarchy",
        "A. F\nB. T",
        "A. Memory speed has exceeded processor speed\nB. Processor speed has grown, but not nearly as much as memory speed\nC. Processor speed has significantly outpaced memory speed\nD. Processor and memory latency have remained relatively constant with respect to each other",
        "A. Put much larger demands on memory access\nB. Don't require as much memory access because of data sharing among cores\nC. Have only two level of cache\nD. Have about the same memory access requirements",
        "A. Two levels of cache per core\nB. Third level of cache shared across all cores\nC. Multi-port, pipelined architecture\nD. Variable front-side bus speed to respond to processor demand",
        "A. Have more cores, but the same size cache\nB. Have more cores and much more L3 cache\nC. Have per-core L1, L2 and L3 cache\nD. Shared L1, L2 and L3 cache",
        "A. There are n sets in which data may be placed\nB. Data may be placed anywhere in cache\nC. Data may be placed in any of the n cache blocks in a set\nD. N blocks can be replaced at once",
        "A. How crucial the data is\nB. The previous block evicted from cache\nC. Which set has room\nD. The address of the requested data",
        "A. The most efficient use of cache memory\nB. One set for all of cache\nC. Multiple blocks per set if the way is greater than 1\nD. One block per set",
        "A. Write out\nB. Write back\nC. Write through\nD. Write left",
        "A. Coordinated\nB. Capacity\nC. Compulsory\nD. Conflict",
        "A. Using non-blocking cache\nB. Executing other instructions while waiting for data\nC. Switching processes\nD. Slowing clock cycle time",
        "A. Fewer conflict misses\nB. More blocks per set\nC. Fewer compulsory misses\nD. Increased hit time",
        "A. Reliability\nB. Speed\nC. Low power consumption\nD. inexpensive in large quantities",
        "A. Reduce power\nB. Increase bandwidth\nC. Decrease latency\nD. Increase speed",       
        "A. Heat problems if it ran faster\nB. Organization on chips\nC. The row/column access method\nD. Refresh cycles",
        "A. Double Data Rate\nB. Multiple banks\nC. Synchronous DRAM\nD. More rows than columns",
        "A. Increasing fault tolerance\nB. Higher transfer rates\nC. Increasing DIMM capacity\nD. Decreasing operating voltage",
        "A. The chips are soldered rather than socketed\nB. The chips are larger\nC. Its operating voltage is below DDR 3's.\nD. Graphics demands more memory",
        "A. Supercooling connections between the processor and DRAM\nB. Packaging it with the CPU\nC. Using exceptionally large bandwidth\nD. Creating smaller DIMM boards",
        "A. Slower than both disk and DRAM\nB. Faster than disk but slower than DRAM\nC. Faster than both disk and DRAM\nD. Faster than DRAM but slower than disk",
        "A. Irregular read rates\nB. Power consumption\nC. The lowest voltage it can operate at\nD. The number of writes to a block",
        "A. Mirroring on flash memory\nB. Duplicating memory\nC. Frequent flushes to disk\nD. Using error correction",
        "A. Increase bandwidth\nB. Reduce hit time\nC. Decrease cache size\nD. Reduce miss penalty",
        "A. Dynamic RAM\nB. Small direct-mapped caches\nC. Small fully associative caches\nD. n-way associative caches",
        "A. 8-way set associative cache\nB. L1 cache\nC. Obtaining data from a register\nD. direct-mapped cache",
        "A. Improves D-cache access with respect to I-cache access\nB. Reduces cache power consumption\nC. Improves cache hit time\nD. It is negligible, so it is seldom used",
        "A. Bandwidth is sacrificed for latency\nB. Has a chance of fetching the wrong data\nC. Increases hit time\n",
        "A. There are as many banks as pipeline stages\nB. Blocks in a set are distributed across the banks\nC. Increasing bandwidth to the processor\nD. Serial access to blocks in a set",
        "A. Services hits while processing a miss\nB. Handles multiple misses transparently\nC. Does not impede the processor\nD. Must be fully associative",
        "A. Critical word first\nB. Early restart\nC. Load-and-go\nD. Priority data",
        "A. The data is written to the write buffer rather than processing a miss in L1\nB. L1 processes a miss and retrieves the evicted block from the write buffer\nC. An exception is thrown\nD. L1 processes a miss and retrieves the evicted block from L2",
        "A. Blocking\nB. Segmenting\nC. Loop interchange\nD. Sematic optimization",
        "A. Only if the programmer supplies prefetch hints\nB. By the compiler only\nC. By the processor only\nD. By both the compiler and the processor",
        "A. By encrypting instructions\nB. By hiding processes from the Operating System\nC. By keeping instructions and data separate\nD. By keeping each process in its own memory space",
        "A. T\nB. F",
        "A. Relocate data in main memory\nB. Aid in translating virtual to physical addresses\nC. Help translate physical to virtual addresses\nD. Load pages from the page file into memory",
        "A. F\nB. T",
        "A. Perform address translation\nB. To manage virtual memory\nC. Allow computer hardware to be shared by many, unrelated users\nD. Ensure that Operating systems work properly",
        "A. Ensures users don't allocate too much memory\nB. makes the user think that there's only one machine running\nC. Allows heterogeneous operating systems to communicate with each other\nD. Allows multiple \"guest\" operating systems to run on the same machine",
        "A. It ensures the most efficient use of main memory\nB. It is unaware of the guest operating systems running on the machine\nC. User applications can control address translation\nD. It reveals only a portion of the real page table to guest OSs",
        "A. Virtual address translation happens more quickly\nB. Intercepting guest OS calls takes less overhead\nC. The processor will run more slowly\nD. It may infringe on IBM's patents",
        "A. Ensure all instructions coming into the processor are legitimate\nB. Fetch exactly one instruction each clock cycle\nC. Decouple instruction fetch from the execution pipeline\nD. Reschedule instructions if it is more optimal to do so",
        "A. Processing stops\nB. Program control jumps immediately to the exception handler\nC. Speculative instructions are not allowed\nD. The exception is buffered until speculation is resolved",
        "A. Detach instruction fetch from pipeline execution\nB. Hold privileged instructions only the OS can use\nC. Speed up execution of small loops\nD. Create a more secure operating environment",
        "A. When the CPU switches processes automatically\nB. When the compiler rearranges the order of instructions\nC. When the CPU rearranges the order of instructions\nD. When the compiler rearranges the layout of memory",
        "A. It creates more exceptions\nB. It causes the CPU to ignore some exceptions\nC. It complicates exception handling\nD. It makes the program run more slowly",
        "A. Longer instruction fetch times\nB. Out of order execution\nC. Out of order exceptions\nD. Excessive CPU idle time",
        "A. Memory reallocation\nB. Register renaming\nC. Better exception handling\nD. Reordering instructions",
        "A. Executes instructions when operands are available\nB. Execute each instruction on a just-in-time basis\nC. To make loops more efficient\nD. The compiler reorders the instructions, the CPU executes them",
        "A. Allocation stations\nB. Reservation stations\nC. Registration stations\nD. Registration allocations",
        "A. Keeping track of the cycle in which an instruction is supposed to execute\nB. Causing fewer cache misses\nC. Increasing clock speed\nD. Redirecting instruction results to other instructions' inputs",
        "A. A result of dynamic scheduling\nB. A product of speculative execution\nC. Two independent uses of the same register\nD. When instructions are reordered and produce the wrong answer",
        "A. Register renaming\nB. Exception handling\nC. Memory allocation\nD. Instruction execution\n",
        "A. Results of execution instruction are immediately stored\nB. Registers are updated after each instruction's execution\nC. Instructions are executed at the last possible moment\nD. Results of executed instructions are broadcast on the common data bus",
        "A. Instruction execute\nB. Instruction issue\nC. Handle exceptions\nD. Write results",
        "A. Its result is written to the register file or memory\nB. It has been scheduled for execution\nC. It will not or cannot be executed\nD. It has been written back to memory",
        "A. It holds instruction results until they are committed\nB. It allows the processor to write values in any order\nC. It obtains new instructions when the old one have been executed\nD. It assures that instructions are executed in the proper order",
        "A. They are written to the mispredict buffer\nB. They are erased from main memory\nC. They are eventually discarded\nD. They are committed",
        "A. Never\nB. When the result is written to the reorder buffer\nC. Only when the instruction is ready to commit\nD. As soon as they occur",
        "A. T\nB. F",
        "A. Increase clock speed\nB. Use statically scheduled superscalar processors\nC. use Very Long Instruction Words  (VLIWs)\nD. Use dynamically scheduled superscalar processors",
        "A. Can contain load/store and arithmetic logic tasks in the same instruction\n	B. Provides more detail on how to execute the instruction\nC. Contains redundant bits to ensure accuracy\nD. Must be 16-byte aligned",
        "A. Multiple Issue\nB. Variable voltage power supply\nC. Dynamic scheduling\nD. So\\peculation",
        "A. F\nB. T\n",
        "A. There can be no dependencies\nB. Dependencies are encoded in reservation stations\nC. Only one reservation station is needed\nD. There can be only one commit",
        "A. The ability to speculate through multiple branches\nB. The ability to jump to the anticipated branch target without calculating it\nC. A more accurate branch predictor\nD. It eliminates the need for branch instructions",
        "A. The instruction at the branch target, avoiding the need to fetch the instruction\nB. The ability to speculate through multiple branches\nC. The ability to go backwards through branches in case a branch is mispredicted\nD. It makes branch prediction tables smaller",
        "A. The branch target instruction may no longer be located in the same place\nB. The branch target instruction may be different from what it was when the\n\tsame branch was previously taken\nC. The branch target may be the same, but the branch target instruction may not be.\nD. Instruction fetch time may be longer because the branch target instruction does not\n\tbenefit from spatial locality",
        "A. Return address predictor\nB. Branch folding table\nC. Branch target buffer\nD. Return stack",
        "A. It will be a very deep stack\nB. Branch prediction is unnecessary\nC. It will predict correctly each time\nD. the CPU will waste power",
        "A. Branch prediction\nB. Instruction prefetch\nC. Instruction memory access and buffering\nD. Return value prediction",
        "A. Branch prediction\nB. Register renaming\nC. Rapid writeback\nD. Instruction fetch",
        "A. It fetches more instructions\nB. It complicates recovery in case of a misprediction.\nC. It fills the data cache with unwanted information\nD. It makes branch predictions less accurate",
        "A. Can waste energy\nB. requires larger instruction caches\nC. Is snot needed with the accuracy of today's branch predictors\nD. Improves performance in almost every application",
        "A. Integer\nB. Floating point\nC. Database\nD. Web", 
        "A. The compiler can understand the sense of the whole program.\nB. There aren't very many lines of code to compile.\nC. Branching is unpredictable or unreliable at compile time.\nD. Integer benchmarks are involved.",
        "A. Neither cache pipeline stalls nor page faults are OK\nB. Cache pipeline stalls are OK\nC. Page faults are OK\nD. Off-chip cache misses are OK",
        "A. Two\nB. One\nC. Four\nD. Three",
        "A. F\nB. T",
        "A. T\nB. F",
        "A. Fine-grained multithreading\nB. Superscalar\nC. Coarse-grained Multithreading\nD. Simultaneous multithreading",
        "A. Coarse-grained Multithreading\nB. Simultaneous multithreading\nC. Fine-grained multithreading\nD. Superscalar",
        "A. T\nB. F",
        "A. Fixes for Java security problems slow down program execution\nB. Multithreaded Java applications exhibit little parallelism\nC. Java has an interpreter for a runtime engine\nD. The Java virtual machine is known to be slow",
        "A. Eight\nB. Eleven\nC. Thirteen\nD. Five",
        "A. Nine\nB. Five\nC. Three\nD. Seven",
    };
    
    private final String[] correctAnswers = {
      "A", "CE", "D", "C", "C", "C", "C", "A", "C", "C", "D", "A", "A", "D",
      "B", "A", "B", "A", "D", "C", "B", "B", "D", "B", "C", "D", "A", "D", "D",
      "B", "D", "D", "D", "A", "D", "A", "C", "B", "B", "A", "A", "A", "A", "B",
      "B", "A", "D", "A", "C", "A", "D", "B", "C", "D", "D", "C", "A", "B", "C",
      "B", "B", "D", "D", "A", "A", "B", "B", "D", "D", "C", "B", "A", "C", "D",
      "B", "A", "A", "A", "A", "D", "D", "A", "B", "A", "C", "D", "D", "B", "C",
      "D", "C", "C", "C", "B", "B", "A", "B", "D", "C", "A", "D", "C", "A", "A",
      "C", "C", "A", "A", "A", "B", "B", "B", "B", "A", "D", "A", "C", "D", "B",
      "B", "A", "A", "C", "A", "A", "A", "B", "B", "B", "A", "B", "A", "B",
    };
    
    public Questions(){}
    
    // Returns questions
    public String[] getQuestions(){
        return questions;
    }
    
    // Returns answers
    public String[] getAnswers(){
        return answers;
    }
    
    // Returns correct answers
    public String[] getCorrectAnswers(){
        return correctAnswers;
    }
}